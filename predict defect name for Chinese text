#################################################
## 使用缺陷描述，预测缺陷部位
#################################################


# get data 
data <- read.csv('defect_name_20161209.csv', stringsAsFactors = FALSE)

# get defect description text 
text <- data$"原句子"


##############################
### segment sentence
##############################
library(jiebaR)
# segment sentence
cutter <- worker(byline = TRUE, 
                 user = "D:/application/R/library/jiebaRD/dict/user.dict.electricity.utf8")
text_seg <- segment(text, cutter)
text_seg[[1]]
text[1]

# get keyword
# modify keyword could modify idf dict 
# keyworker = worker("keywords", 
#                    idf = "D:/application/R/library/jiebaRD/dict/idf.electricity.utf8")
keyworker = worker("keywords")
keyword <- lapply(text_seg, function(x) {
  vector_keywords(x, keyworker)
})

keyword[[1]]
text_seg[[2]]
text[2]
keyword[[2]]

# segment and get keyword 
# not use user dict
simhasher = worker("simhash",topn=5)
simhasher <= text[1]
distance(text[1], text[2], simhasher)


##############################
### construct a document-term matrix
##############################
library(text2vec)

# random seed
set.seed(2016L)
# split data to train and test 
all_ids <- 1 : length(text)
train_ids <- sample(all_ids, round(length(text) * 0.8))
test_ids <- setdiff(all_ids, train_ids)
train <- text_seg[train_ids]
test <- text_seg[test_ids]

# vocabulary-based vectorization
it_train <- itoken(train, progressbar = FALSE)
vocab = create_vocabulary(it_train)
vocab

# construct a document-term matrix
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)


##############################
### fit a model
##############################
# fit a logistic regression model 
# with an L1 penalty and 4 fold cross-validation

# delete space in the word and transform character to factor
y <- factor(gsub(' ', '', data[['缺陷部位1']]))
levels(y)
y_n <- as.numeric(y)

y_train <- y_n[train_ids]
y_test <- y_n[test_ids]

library(glmnet)
glmnet_classifier <- cv.glmnet(x = dtm_train, y = y_train,
                               family = 'multinomial', 
                               type.measure = 'mse', 
                               nfolds = 3)
print(paste("min Mean Squared Error =", round(min(glmnet_classifier$cvm), 4)))
# "min Mean Squared Error = 0.1331"

# glmnet_classifier <- cv.glmnet(x = dtm_train, y = y_train,
#                                family = 'multinomial', 
#                                type.measure = 'class', 
#                                nfolds = 3)
# print(paste("min misclassification Error =", round(min(glmnet_classifier$cvm), 4)))
# # "min misclassification Error = 0.0715"
# 1 - min(glmnet_classifier$cvm)
# # 0.9285177

# check the model's performance on test data 
# Note that most text2vec functions are pipe friendly!
it_test = test %>% 
  itoken(progressbar = FALSE)

dtm_test = create_dtm(it_test, vectorizer)

preds = predict(glmnet_classifier, dtm_test, type = 'class')[, 1]
sum(y_test == as.numeric(preds))/length(preds)
# 0.936747
table(y_test, as.numeric(preds))
# y   1   2   3   4   5
# 1 126   0   1   2   1
# 2   2  33   0   0   1
# 3   3   0  59   0   0
# 4   3   0   0  50   0
# 5   8   0   0   0  43

preds = predict(glmnet_classifier, dtm_test, type = 'class', 
                s = glmnet_classifier$lambda.1se)[, 1]
sum(y_test == as.numeric(preds))/length(preds)

logis <- glmnet(x = dtm_train, y = y_train, 
                family = 'multinomial')

pred_train <- predict(logis, dtm_train, type = 'class')[, 1]
sum(y_train == as.numeric(pred_train))/length(pred_train)
# 0.34
pred_test <- predict(logis, dtm_test, type = 'class', 
                     s = glmnet_classifier$lambda.1se)[, 1]
sum(y_test == as.numeric(pred_test))/length(pred_test)
# 0.39

glmnet_classifier$lambda.1se
[1] 0.01006668
glmnet_classifier$lambda.min
[1] 0.004159563

############################################################################################

#########################    1. segment the text ###########################
library(jiebaR)

f_segment_ch <- function(text, type = "mix", 
                         user_dict_path = "D:/application/R/library/jiebaRD/dict/user.dict.electricity.utf8",
                         stop_word_path = "rename the file in the STOPPATH") {
  
  # segment sentence
  cutter <- worker(type = type, byline = TRUE, 
                   user = user_dict_path, 
                   stop_word = stop_word_path)
  segment(text, cutter)

}


#########################    2. train the models #########################
f_logis_save <- function(dataset, x_col_name, y_col_name, model_name) {
  
  tryCatch(
    expr = {
      
      library(text2vec)
      library(glmnet)
      
      # choose data 
      data <- read.csv(dataset, stringsAsFactors = FALSE)
      #  data <- read.csv('defect_name_20161209.csv', stringsAsFactors = FALSE)
      # choose x col
      #  text = data$"原句子"
      text <- subset(data, select = x_col_name, drop = TRUE)
      # chooose y col 
      #  y_col = data[['缺陷部位3']]
      y_col = subset(data, select = y_col_name, drop = TRUE)
      
      
      #  deal with few y value which frequency < = min_num
      min_num <- round(nrow(data) * 0.01)
      temp_y <- table(y_col)
      add_y_name <- names(temp_y)[unname(temp_y) < min_num]
      # add new cols 
      new_data <- data
      for(i in add_y_name) {
        short_index <- which(y_col == i)
        add_index <- rep(short_index, length.out = min_num)
        new_data <- rbind(new_data, data[add_index, ])
      }
      
      # renew x y col
      # text = new_data$"原句子"
      # y_col = new_data[['缺陷部位3']]
      text <- subset(new_data, select = x_col_name, drop = TRUE)
      y_col = subset(new_data, select = y_col_name, drop = TRUE)
      
      # load R function
      source('f_segment_ch.R')
      # segment sentence
      train <- f_segment_ch(text = text)
      # vocabulary-based vectorization
      it_train <- itoken(train, progressbar = FALSE)
      vocab = create_vocabulary(it_train)
      # construct a document-term matrix
      vectorizer <- vocab_vectorizer(vocab)
      dtm_train <- create_dtm(it_train, vectorizer)
      
      
      # y variable
      # delete space in the y value 
      y <- factor(gsub(' ', '', y_col))
      y_list <- levels(y)
      y_train <- as.numeric(y)
      
      
      ####################################
      ### logis train
      # train the model
      glmnet_classifier <- cv.glmnet(x = dtm_train, 
                                     y = y_train,
                                     family = 'multinomial', 
                                     type.measure = 'class', 
                                     nfolds = 3)
      
      save(vectorizer, y_list, glmnet_classifier, file = model_name)
      
      list("Accuracy value" = 1 - round(min(glmnet_classifier$cvm), 4))
      
    },
    
    error = function(e) e
    
  )
}

###############################      3. prediction     ###############################################

library(jiebaR)
library(text2vec)
library(glmnet)


f_old_logis_pred <- function(new_text, model_name) {
  
  # load R function
  source('f_segment_ch.R')
  # segment sentence
  test <- f_segment_ch(text = new_text)
  
  # test data 
  it_test = test %>% 
    itoken(progressbar = FALSE)
  
  # load trained model and related file, such as vectorizer (vacabulary by trainset)
  # and y_list (levels(y) by train y col)
 
  #if (model_type == 1) {
   # load("logis_model.RData")
  #} else if(model_type == 2) {
   # load("logis_model_2.RData")
  #} else if(model_type == 3) {
   # load("logis_model_3.RData")
  #}
  
  load(model_name)
  
  dtm_test = create_dtm(it_test, vectorizer)
  
  # predict y value in the test set
  preds = predict(glmnet_classifier, dtm_test, type = 'class', 
                  s = "lambda.1se")[, 1]
  
  y_list[as.numeric(preds)]
  
}

########################################## 4 use other models #########################################

library(randomForest)
library(gbm)

kinds_model <- function(text, y_col) {
  # text = raw_text
  # delete spaces
  y_col = gsub(' ', '', y_col)
  # min number per subclass
  min_num <- ceiling(length(y_col)*6/1000)
  
  min_num = ifelse(min_num < 9, 9, min_num)
  
  # deal with few subclass
  if(length(which(table(y_col) < min_num)) != 0) {
    # find which name has few times
    add_item <- names(which(table(y_col) < min_num))
    # find index which have few names
    add_index = unlist(lapply(add_item, function(x) which(y_col == x)[1]))
    # append x and y rows which has few times
    text = append(text, text[rep(add_index, each = min_num)])
    y_col = append(y_col, y_col[rep(add_index, each = min_num)])
  }
  
  y <- factor(y_col)
  y_list <- levels(y)
  y_train <- as.numeric(y)
  
  # load R function
  source('f_segment_ch.R')
  # segment sentence
  train <- f_segment_ch(text = text)
  
  # vocabulary-based vectorization
  it_train <- itoken(train, progressbar = FALSE)
  vocab = create_vocabulary(it_train)
  # construct a document-term matrix
  vectorizer <- vocab_vectorizer(vocab)
  dtm_train <- create_dtm(it_train, vectorizer)
  
  data_pred <- list("dtm_train" = dtm_train, "y_train" = y_train, "y_list" = y_list)
  
  glmnet_classifier <- cv.glmnet(x = data_pred[["dtm_train"]], 
                                 y = data_pred[["y_train"]],
                                 family = 'multinomial', 
                                 type.measure = 'class', 
                                 nfolds = 3)
  glmnet_classifier
  
  
  #############
  data_pred <- list("dtm_train" = as.matrix(dtm_train), "y_train" = y, "y_list" = y_list)
  
  randomForest_classifier <- randomForest(x = data_pred[["dtm_train"]], 
                                          y = data_pred[["y_train"]])
  
  randomForest_classifier
  
  ####################  boosting
  data_df <- data.frame(as.matrix(dtm_train))
  data_df['y'] <- y
  
  boost_classifier <- gbm(y ~ ., data = data_df, cv.folds = 3, 
                          distribution = "multinomial")
  boost_classifier
  summary(boost_classifier) 
  
  ################## artificial neural networks
  bayes_classifier <- naiveBayes(as.matrix(dtm_train), y)
  summary(bayes_classifier)
  
  
}







