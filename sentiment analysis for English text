#####################################################
### sentiment analysis
### use movie_review dataset
### it consists of 5000 movie reviews, 
### each of which is marked as positive or negative
### http://text2vec.org/vectorization.html
####################################################

library(text2vec)
library(data.table)
library(glmnet)

data("movie_review")
# convert list and data.frame to data.table
setDT(movie_review)
# sort data table
setkey(movie_review, id)

# random seed
set.seed(2016L)
# split data to train and test 
all_ids <- movie_review$id
train_ids <- sample(all_ids, 4000)
test_ids <- setdiff(all_ids, train_ids)
train <- movie_review[J(train_ids)]
test <- movie_review[J(test_ids)]


# vocabulary-based vectorization
it_train <- itoken(train$review, 
                   preprocessor = tolower, 
                   tokenizer = word_tokenizer, 
                   ids = train$id,
                   progressbar = FALSE)
vocab <- create_vocabulary(it_train)
vocab

# construct a document-term matrix
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
dim(dtm_train)
# 4000 35596

# fit a logistic regression model 
# with an L1 penalty and 4 fold cross-validation
glmnet_classifier <- cv.glmnet(x = dtm_train, y = train[['sentiment']],
                               family = 'binomial',
                               alpha = 1,# L1 penalty
                               type.measure = "auc",
                               nfolds = 4,
                               # high vlue is less accurate, but has faster training
                               thresh = 1e-3,
                               # again lower number of iterations for faster training
                               maxit = 1e3)
plot(glmnet_classifier)

print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# "max AUC = 0.9236"


# check the model's performance on test data 
# Note that most text2vec functions are pipe friendly!
it_test = test$review %>% 
  tolower %>% 
  word_tokenizer %>% 
  itoken(ids = test$id, 
         progressbar = FALSE)

dtm_test = create_dtm(it_test, vectorizer)

preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
# 0.916325


#############################
### pruning vocabulary
############################
stop_words <- c('i', 'me', 'my', 'myself', 'we', 'our', 'ours', 
                'ourselves', 'you', 'your', 'yours')
vocab <- create_vocabulary(it_train, stopwords = stop_words)

pruned_vocab = prune_vocabulary(vocab, 
                                term_count_min = 10, 
                                doc_proportion_max = 0.5,
                                doc_proportion_min = 0.001)
vectorizer = vocab_vectorizer(pruned_vocab)
# create dtm_train with new pruned vocabulary vectorizer
dtm_train  = create_dtm(it_train, vectorizer)
dim(dtm_train)
# 4000 6585

it_test = test$review %>% 
  tolower %>% 
  word_tokenizer %>% 
  itoken(ids = test$id, 
         progressbar = FALSE)
dtm_test   = create_dtm(it_test, vectorizer)
dim(dtm_test)
# 1000 6585

glmnet_classifier <- cv.glmnet(x = dtm_train, y = train[['sentiment']],
                               family = 'binomial',
                               alpha = 1,# L1 penalty
                               type.measure = "auc",
                               nfolds = 4,
                               # high vlue is less accurate, but has faster training
                               thresh = 1e-3,
                               # again lower number of iterations for faster training
                               maxit = 1e3)
plot(glmnet_classifier)

print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# "max AUC = 0.9209"
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
# 0.9180611


#############################
### N-grams
############################
vocab <- create_vocabulary(it_train, ngram = c(1L, 2L))
vocab
# Number of docs: 4000 
# 0 stopwords:  ... 
# ngram_min = 1; ngram_max = 2 
# Vocabulary: 
#   terms terms_counts doc_counts
# 1:           to_times            1          1
# 2:           backs_to            1          1
# 3:           few_step            1          1
# 4:          also_took            1          1
# 5:        in_graphics            1          1
# ---                                           
#   397499:           br_funny            2          2
# 397500:    dealings_became            1          1
# 397501:          erase_the            2          2
# 397502:          became_so            2          2
# 397503: blockbuster_wanted            1          1

vocab = vocab %>% prune_vocabulary(term_count_min = 10, 
                                   doc_proportion_max = 0.5)

bigram_vectorizer = vocab_vectorizer(vocab)

dtm_train = create_dtm(it_train, bigram_vectorizer)
dim(dtm_train)
# 4000 17870

glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']], 
                              family = 'binomial', 
                              alpha = 1,
                              type.measure = "auc",
                              nfolds = 4,
                              thresh = 1e-3,
                              maxit = 1e3)
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# "max AUC = 0.9216"

dtm_test = create_dtm(it_test, bigram_vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
# 0.9268334

#############################
### Feature hashing
# As you can see our AUC is a bit worse but DTM construction time 
# is considerably lower. 
# On large collections of documents this can be 
# a significant advantage.
############################
h_vectorizer = hash_vectorizer(hash_size = 2^14, ngram = c(1L, 2L))
t1 = Sys.time()
dtm_train = create_dtm(it_train, h_vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))

dim(dtm_train)
# 4000 16384

glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['sentiment']], 
                              family = 'binomial', 
                              alpha = 1,
                              type.measure = "auc",
                              nfolds = 5,
                              thresh = 1e-3,
                              maxit = 1e3)
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# "max AUC = 0.8999"

dtm_test = create_dtm(it_test, h_vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
# 0.9034285


#############################
### Basic transformations
#############################

### Normalization
dtm_train_l1_norm = normalize(dtm_train, "l1")

### TF-IDF
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
dtm_train = create_dtm(it_train, vectorizer)
# train
tfidf = TfIdf$new()
dtm_train_tfidf = fit_transform(dtm_train, tfidf)
# test
dtm_test_tfidf  = create_dtm(it_test, vectorizer) %>% 
  transform(tfidf)
# model
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train_tfidf, y = train[['sentiment']], 
                              family = 'binomial', 
                              alpha = 1,
                              type.measure = "auc",
                              nfolds = 4,
                              thresh = 1e-3,
                              maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
# Time difference of 3.735374 secs
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
# "max AUC = 0.9129"

preds = predict(glmnet_classifier, dtm_test_tfidf, type = 'response')[,1]
glmnet:::auc(test$sentiment, preds)
# 0.9062926
